llm:
  chat_endpoints:
    storyteller_default:
      model: "deepseek-v3.2"
      temperature: 0.35
      max_concurrency: 8
  embedding_endpoints:
    embedding_default:
      model: "text-embedding-3-large"
      max_concurrency: 10

storyteller:
  narration_ratio: [0.35, 0.45]
  narration_temperature: 0.35
  memory_top_k: 4

  # Optional step mode (uncomment to enable)
  # step_size: 5
  # step_memory_mode: "per_chapter"
  # step_retrieval_concurrency: 10
